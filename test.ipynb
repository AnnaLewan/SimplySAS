{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "from Bio import pairwise2\n",
    "import zipfile\n",
    "import tarfile\n",
    "from collections import defaultdict\n",
    "import uuid\n",
    "import subprocess\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_multifile = 'Salmo_MHC_I.zip'\n",
    "INP_expected_length = 214\n",
    "INP_substitution_error_threshold = 1\n",
    "INP_min_amplicon_seq_frequency = 0.25\n",
    "INP_min_dominant_frequency_threshold = 25\n",
    "\n",
    "INP_min_chimera_length = 10\n",
    "INP_max_allele_number = 50  #Do weryfikacji\n",
    "INP_min_amplicon_depth = 100 ######################## NAPISAĆ DO TEGO FUNKCJĘ\n",
    "\n",
    "INP_min_per_amplicon_frequency = 10 #Do weryfikacji\n",
    "INP_discard_noncoding = 1 #(1 wykonaj)\n",
    "\n",
    "p1 = f'ampliSAS_analysis'\n",
    "os.makedirs(p1)\n",
    "\n",
    "p2 = f'ampliSAS_analysis/input'\n",
    "os.makedirs(p2)\n",
    "\n",
    "if tarfile.is_tarfile(input_multifile) or zipfile.is_zipfile(input_multifile) != True:\n",
    "    raise Exception('File is not a multifile')\n",
    "            \n",
    "    \n",
    "### Sprawdzanie parametrów\n",
    "if INP_substitution_error_threshold == None:\n",
    "    substitution_error_threshold = 1\n",
    "else:\n",
    "    substitution_error_threshold = INP_substitution_error_threshold\n",
    "\n",
    "    \n",
    "if INP_min_amplicon_seq_frequency == None:\n",
    "    min_amplicon_seq_frequency = 0.25\n",
    "else:\n",
    "    min_amplicon_seq_frequency = INP_min_amplicon_seq_frequency\n",
    "    \n",
    "\n",
    "if INP_min_dominant_frequency_threshold == None:\n",
    "    min_dominant_frequency_threshold = 25\n",
    "else:\n",
    "    min_dominant_frequency_threshold = INP_min_dominant_frequency_threshold\n",
    "    \n",
    "    \n",
    "if INP_min_chimera_length == None:\n",
    "    min_chimera_length = 10\n",
    "else:\n",
    "    min_chimera_length = INP_min_chimera_length\n",
    "    \n",
    "    \n",
    "if INP_max_allele_number == None:\n",
    "    max_allele_number = 50\n",
    "else:\n",
    "    max_allele_number = INP_max_allele_number\n",
    "    \n",
    "    \n",
    "if INP_min_amplicon_depth == None:\n",
    "    min_amplicon_depth = 100\n",
    "else:\n",
    "    min_amplicon_depth = INP_min_amplicon_depth\n",
    "    \n",
    "    \n",
    "if INP_min_per_amplicon_frequency == None:\n",
    "    min_per_amplicon_frequency = 10\n",
    "else:\n",
    "    min_per_amplicon_frequency = INP_min_per_amplicon_frequency\n",
    "\n",
    "    \n",
    "if INP_discard_noncoding == None:\n",
    "    discard_noncoding = None\n",
    "else:\n",
    "    discard_noncoding = INP_discard_noncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fastqs from multifile\n",
      "Extraction finished\n"
     ]
    }
   ],
   "source": [
    "os.mkdir(f'{p2}/fastqs')\n",
    "print(f'Extracting fastqs from multifile')\n",
    "\n",
    "#for i in input_multifile:\n",
    "with zipfile.ZipFile(input_multifile, 'r') as zip:\n",
    "    zip.extractall(path = f'{p2}')\n",
    "        \n",
    "print(f'Extraction finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Amplicon:\n",
    "    def __init__(self, number):\n",
    "        self.number = number\n",
    "        self.clusters = None\n",
    "        \n",
    "        \n",
    "#     def cut_primers(self, input_fastq, outputs_dir_good, outputs_dir_bad, starter_f, starter_r):\n",
    "#         command = f'python3 cutPrimers.py -r1 {input_fastq} -pr15 {starter_f} -pr13 {starter_r} -tr1 {outputs_dir_good}/amplicon{self.number}_primerfree.fastq -utr1 {outputs_dir_bad}/odrzucone_ampli{self.number}.fastq'        \n",
    "#         process = subprocess.check_output(command, shell=True)\n",
    "        \n",
    "            \n",
    "    def parse_sequence_file(self, inputfastq):\n",
    "        global amplicon_table\n",
    "        amplicon_table = pd.DataFrame(columns = ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'])      ## ogarnąc czy przypadkiem nie trzeba zrobić w inicie  \n",
    "        \n",
    "        dedup_records = defaultdict(list) #tworzy słownik, dla których wartościami jest lista\n",
    "\n",
    "        for record in SeqIO.parse(inputfastq, \"fastq\"):\n",
    "            dedup_records[str(record.seq)].append(record.id) #tworzy słownik, dla którego key=seq, a value=id. Na tym etapie dochodzi do odnalezienia duplikatów - jest ich tyle wartości id\n",
    "        for seq, ids in sorted(dedup_records.items(), key=lambda t: len(t[1]), reverse=True): #ta linijka służy przesortowaniu sekwencji pod względem ich głębokości                \n",
    "            id_hold = ids[0]\n",
    "            id_len = len(ids)\n",
    "            seq_len = len(seq)\n",
    "            unique_hash = str(uuid.uuid1())\n",
    "            amplicon_table = amplicon_table.append({'Hash' : unique_hash, 'ID' : id_hold, 'Sequence' : seq, 'Depth' : id_len, 'Length' : seq_len}, ignore_index = True)\n",
    "        \n",
    "        #seqs_count = len(amplicon_table) # Ilość unikalnych sekwencji w amplikonie\n",
    "        depth_of_amplicon = amplicon_table['Depth'].sum() #Wylicza głębie całego amplikonu\n",
    "        \n",
    "        amplicon_table['Frequency'] = amplicon_table['Depth']/depth_of_amplicon*100\n",
    "        \n",
    "#         for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "#             freq_in_amplicon = row.Depth/depth_of_amplicon*100\n",
    "#             row.Frequency = freq_in_amplicon\n",
    "                    \n",
    "        return amplicon_table\n",
    "\n",
    "    \n",
    "    def find_minmax(self):\n",
    "        if INP_expected_length == None:\n",
    "            expected_length = amplicon_table.loc[0, 'Length']\n",
    "        else:\n",
    "            expected_length = INP_expected_length\n",
    "        \n",
    "        max_length = expected_length + 15\n",
    "        min_length = expected_length - 15\n",
    "        \n",
    "        amplicon_table.drop(amplicon_table[amplicon_table.Length < min_length].index, inplace=True)  #odrzuca sekwencje o długości niższej niż zadana przez użytkownika\n",
    "        amplicon_table.drop(amplicon_table[amplicon_table.Length > max_length].index, inplace=True)  #odrzuca sekwencje o długości wyższej niż zadana przez użytkownika\n",
    "        amplicon_table.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    \n",
    "    def create_fasta(self, inputs_dir):\n",
    "        outpath = f'{inputs_dir}/{self.number}_amplicon.fasta'\n",
    "        with open(outpath, 'w') as outfile:\n",
    "            for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "                outfile.write('>' + row.Hash + ' | ' + row.ID + ' | depth: ' + str(row.Depth) + ' | length: ' + str(row.Length) + ' | frequency per amplicon: ' + str(row.Frequency) + '\\n' + row.Sequence + '\\n')\n",
    "                \n",
    "                \n",
    "    def create_seq_fasta(self, seqs_fasta_dir):\n",
    "        for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "            outpath = f'{seqs_fasta_dir}/{row.Hash}.fasta'\n",
    "            with open(outpath, 'w') as outfile:\n",
    "                outfile.write('>' + row.Hash + ',' + row.ID + ',' + str(row.Depth) + ',' + str(row.Length) + ',' + str(row.Frequency) + '\\n' + row.Sequence + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, amplicon):\n",
    "        self.dominant_seq = None\n",
    "        self.amplicon = amplicon\n",
    "                               \n",
    "                \n",
    "    def seq2seq(self, seq1, seq2, substitution_error_threshold, number):\n",
    "        command = f'./fogsaa {seq1} {seq2} 1 0 1 -1 -1'\n",
    "        process = subprocess.check_output(command, shell=True) #Przykładowy wynik: b'176\\n176\\nElapsed time: 1 milliseconds\\ntotal nodes expanded==176\\n\\nscore= 174\\n'\n",
    "        process = process.decode('utf-8') #Zamiana bajtów na str\n",
    "\n",
    "#         outpath = f'{temp_dir}/{number}.txt'\n",
    "#         with open(outpath, 'w') as outfile:\n",
    "#             outfile.write(process)\n",
    "                \n",
    "        score = int(re.sub('score=\\ ', '', ' '.join(map(str, re.findall('score=\\ .[0-9]*', process))))) # Wyodrębnienie score dla alignmentu\n",
    "        len_seqs = int(re.sub('score=\\ ', '', ' '.join(map(str, re.findall('(?<=[0-9]\\s)[0-9]+', process))))) # Wyodrębnia długość porównywanych sekwencji - założenie, że porównywane sekwencje są tylko tej samej długości\n",
    "        \n",
    "        # Duży problem a propos wyliczenia substitution_error. Fogsaa nie zwraca jaka część score to kara za mismatch.\n",
    "        # Oznacza to, że nie wiem ile substytucji tak naprawdę zaszło, a bez tego nie mogę wyliczyć błędu substytucji.\n",
    "        \n",
    "        global substitution_error\n",
    "        substitution_error = (len_seqs - score)/len_seqs*100 # Wylicza poziom błedów dla danego alignmentu\n",
    "        \n",
    "        \n",
    "    def multiple_aline_seqs(self, cluster_df, clusters_dir, number):\n",
    "        #Tworzenie wspólnej fasty dla klastra\n",
    "        outpath = f'{clusters_dir}/{number}_cluster.fasta'\n",
    "        with open(outpath, 'w') as outfile:\n",
    "            for index, row in cluster_df[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "                outfile.write('>' + row.Hash + ' | ' + row.ID + ' | depth: ' + str(row.Depth) + ' | length: ' + str(row.Length) + ' | frequency per amplicon: ' + str(row.Frequency) + '\\n' + row.Sequence + '\\n')\n",
    "                        \n",
    "        \n",
    "        # Stworzenie dataframe na sekwencję konsensusową z danymi\n",
    "        global consensus_df\n",
    "        \n",
    "        #cluster_df.reset_index(drop=True, inplace=True)\n",
    "#         out = f'{clusters_dir}/klaster.csv'\n",
    "#         cluster_df.to_csv(out)\n",
    "#         print(cluster_df)\n",
    "        \n",
    "        consensus_df = pd.DataFrame(columns = ['ID', 'Sequence', 'Depth', 'Length'])\n",
    "        id_cluster = f'cluster_{number}'\n",
    "        len_cluster = cluster_df.iloc[0, 4]\n",
    "        depth_cluster = cluster_df['Depth'].sum()\n",
    "               \n",
    "        #Ustalenie konsensusowej\n",
    "        align = AlignIO.read(outpath, 'fasta')\n",
    "        summary_align = AlignInfo.SummaryInfo(align)\n",
    "        consensus = str(summary_align.dumb_consensus())\n",
    "    #    my_pssm = summary_align.pos_specific_score_matrix(consensus,chars_to_ignore = ['N']) # Matrix z częstością dla danej pozycji \n",
    "    \n",
    "        consensus_df = consensus_df.append({'ID' : id_cluster, 'Depth' : depth_cluster, 'Length' : len_cluster, 'Sequence' : consensus}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p7 = f'{p2}/fasta'\n",
    "os.mkdir(p7)\n",
    "\n",
    "p8 = f'{p1}/amplicons_filtered_seqs' # Tu będa znajdowały się foldery dla każdego amplikonu zawierające każdą sekwencję w amplikonie jako osobny plik fasta. Sekwencje te są już przefiltrowane względem długości, głebi itd\n",
    "os.mkdir(p8)\n",
    "\n",
    "p9 = f'{p1}/clusters'\n",
    "os.mkdir(p9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files = 0\n",
    "for filename in os.scandir(p2):\n",
    "    if filename.is_file():\n",
    "        files = files+1\n",
    "        go = Amplicon(files)\n",
    "        go.parse_sequence_file(filename.path)\n",
    "        go.find_minmax()\n",
    "        go.create_fasta(p7)\n",
    "        \n",
    "        seqs_fasta_dir = f'{p8}/amplicon_{files}'\n",
    "        os.mkdir(seqs_fasta_dir)\n",
    "        go.create_seq_fasta(seqs_fasta_dir) #Tworzy fasty dla każdej sekwencji w amplikonie\n",
    "        \n",
    "        clusters_dir = f'{p9}/amplicon_{files}'\n",
    "        os.mkdir(clusters_dir)\n",
    "        \n",
    "        do = Cluster(files)   #Wczytanie pliku z funkcjami klastrowania\n",
    "        \n",
    "        all_clusters = pd.DataFrame(columns = ['ID', 'Sequence', 'Depth', 'Length'])\n",
    "        \n",
    "        amplicon_table['state'] = 'U' #Dodanie kolumny 'state'\n",
    "        \n",
    "        index_list = list(amplicon_table.index.values)\n",
    "        \n",
    "        for index in index_list:\n",
    "            if amplicon_table.at[index, 'state'] == 'U':\n",
    "#             #if row.state == 'U':\n",
    "#                 print(index)\n",
    "#                 print(row.state)\n",
    "                if amplicon_table.at[index, 'Frequency'] >= min_amplicon_seq_frequency:\n",
    "                    cluster_pd = pd.DataFrame(columns = ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'])\n",
    "#                 global consensus_df\n",
    "#                 consensus_df = pd.DataFrame(columns = ['ID', 'Sequence', 'Depth', 'Length'])\n",
    "            \n",
    "                    dom_seq_row = amplicon_table.iloc[index]\n",
    "                    dom_seq_hash = amplicon_table.at[index, 'Hash']\n",
    "                    dom_seq_length = amplicon_table.at[index, 'Length']\n",
    "                    dom_seq_depth = amplicon_table.at[index, 'Depth']\n",
    "            \n",
    "                    seq1 = f'{seqs_fasta_dir}/{dom_seq_hash}.fasta'\n",
    "            \n",
    "                    cluster_pd = cluster_pd.append(dom_seq_row)\n",
    "                    \n",
    "#                     out = f'{clusters_dir}/klaster{index}prew.csv'\n",
    "#                     cluster_pd.to_csv(out)\n",
    "                    \n",
    "#                     print(index)\n",
    "                    amplicon_table.at[index, 'state'] = 'C'\n",
    "#                     b=amplicon_table.at[index, 'state']\n",
    "                    \n",
    "#                     print(f'Dom: {b}') ## Dlaczego na tym etapie nie jest rozpoznawane jako C ????\n",
    "                    \n",
    "                    index_klastra = index\n",
    "            \n",
    "                    for index in index_list:\n",
    "#                         z = amplicon_table.at[index, 'state']\n",
    "#                         print(z)\n",
    "                        if amplicon_table.at[index, 'state'] == 'U':\n",
    "#                             c=amplicon_table.at[index, 'state']\n",
    "                            \n",
    "#                             print(f'Początek 2 pętli: {c}')\n",
    "                            #print(cluster_pd)\n",
    "                            #break\n",
    "                        #if row.state == 'U':\n",
    "                            if amplicon_table.at[index, 'Length'] == dom_seq_length:\n",
    "                                seq2_hash = amplicon_table.at[index, 'Hash']\n",
    "                                seq2 = f'{seqs_fasta_dir}/{seq2_hash}.fasta'           \n",
    "                                do.seq2seq(seq1, seq2, substitution_error_threshold, index)\n",
    "#                                 print('Wyjście po długości')\n",
    "                                \n",
    "                    \n",
    "                                if substitution_error < substitution_error_threshold:\n",
    "                                    freq_to_dom = amplicon_table.at[index, 'Depth']/dom_seq_depth*100\n",
    "#                                     print(\"Wyjście na substytucjach\")\n",
    "                                    \n",
    "                        \n",
    "                                    if freq_to_dom < min_dominant_frequency_threshold:\n",
    "                                #print(amplicon_table.iloc[index])\n",
    "\n",
    "                                        cluster_pd = cluster_pd.append(amplicon_table.loc[index]) #dodaje sekwencję do klastra dominującego\n",
    "                                        amplicon_table.at[index, 'state'] = 'C'\n",
    "#                                         d = amplicon_table.at[index, 'state']\n",
    "#                                         print(f'koniec 2 pętli: {d}')\n",
    "#                                         out = f'{clusters_dir}/klaster{index}.csv'\n",
    "#                                         cluster_pd.to_csv(out)\n",
    "                                        \n",
    "                                #amplicon_table.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                                ### Bez else, bo moje rozumowanie jest takie, że i tak każda sekwencja której nie można dodać do klastra będzie\n",
    "### rozważana jako potencjalna dominująca\n",
    "#                     print(cluster_pd)\n",
    "#                     print(index)\n",
    "#                     print(index_klastra)\n",
    "                    do.multiple_aline_seqs(cluster_pd, clusters_dir, index_klastra)\n",
    "                               \n",
    "\n",
    "                    all_clusters = all_clusters.append(consensus_df.iloc[0])\n",
    "        \n",
    "        #Stworzenie ostatecznego pliku fasta z sekwencjami konsensusowymi dla danego amplikonu\n",
    "        \n",
    "        outpath = f'{clusters_dir}/consensus_seqs.fasta'\n",
    "        with open(outpath, 'w') as outfile:\n",
    "            for index, row in all_clusters[ ['ID', 'Sequence', 'Depth', 'Length'] ].iterrows():\n",
    "                outfile.write('>' + row.ID + ' | depth: ' + str(row. Depth) + ' | length: ' + str(row.Length) + '\\n' + row.Sequence + '\\n')\n",
    "                \n",
    "        # Stworzenie pliku csv z sewkencjami konsenusowymi\n",
    "        outpath2 = f'{clusters_dir}/consensus_seqs.csv'\n",
    "        all_clusters.to_csv(outpath2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INP_min_chimera_length = 10\n",
    "# INP_max_allele_number = 50\n",
    "# INP_min_amplicon_depth = 100\n",
    "\n",
    "# INP_min_per_amplicon_frequency = 10 #Do weryfikacji\n",
    "# INP_commonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def noncoding(self, cons_seqs_df):\n",
    "        pattern1 = '[ATCG]*TAG[ATCG]*'\n",
    "        pattern2 = '[ATCG]*TAA[ATCG]*'\n",
    "        pattern3 = '[ATCG]*TGA[ATCG]*'\n",
    "        patterns = [pattern1, pattern2, pattern3]\n",
    "        \n",
    "        index_values = list(cons_seqs_df.index.values)\n",
    "        \n",
    "        for index in index_values:\n",
    "            for pattern in patterns:\n",
    "                seq = cons_seqs_df.at[index, 'Sequence']\n",
    "                checked_seq = re.sub(pattern, '', seq)\n",
    "                cons_seqs_df.at[index, 'Sequence'] = checked_seq        \n",
    "        \n",
    "        cons_seqs_df.drop(cons_seqs_df[cons_seqs_df.Sequence == ''].index, inplace=True)\n",
    "        cons_seqs_df.reset_index(drop=True)\n",
    "\n",
    "        \n",
    "    def is_chimera(self, cons_seqs_df):\n",
    "        seqs_count = len(cons_seqs_df)\n",
    "        cons_seqs_df['Chimera'] = 'Not'\n",
    "        if seqs_count >= 3:\n",
    "            reversed_df = cons_seqs_df.sort_values(['Depth'])\n",
    "            \n",
    "            indexer = [i for i in range(0, seqs_count)]\n",
    "            #Do ustalenia\n",
    "            identity_threshold = 90\n",
    "            \n",
    "            \n",
    "            for j in indexer:\n",
    "                for i in indexer:\n",
    "                    seq1 = str(reversed_df.at[j, 'Sequence'])\n",
    "                    seq2 = str(cons_seqs_df.at[i, 'Sequence'])\n",
    "                    alignment = str(pairwise2.align.globalxx(seq1, seq2, one_alignment_only=True))\n",
    "                    search1 = re.search('(?<=seqB=\\')[ACTG\\-]+\\'', alignment)\n",
    "                    search2 = re.search('(?<=score=)[0-9]+', alignment)\n",
    "                    search3 = re.search('(?<=end=)[0-9]+', alignment)\n",
    "                    aligned_seq = search1.group(0)\n",
    "                    score = int(search2.group(0))\n",
    "                    align_length = int(search3.group(0))\n",
    "                    \n",
    "                    #Wylicza identity\n",
    "                    identity = score/align_length*100\n",
    "                    if identity < identity_threshold:\n",
    "                        seq2_list = list(aligned_seq)\n",
    "                        # Tworzy binary score\n",
    "                        binary_score = []\n",
    "                        for base in seq2_list:\n",
    "                            if base == 'A' or base == 'T' or base == 'C' or base == 'G':\n",
    "                                binary_score.append(1)\n",
    "                            else:\n",
    "                                binary_score.append(0)\n",
    "                        \n",
    "                        binary_score_right_end = binary_score[:min_chimera_length] #Wycinek długości chimera_length bp z prawej\n",
    "                        binary_score_left_end = binary_score[-min_chimera_length:] #Wycinek długości chimera_length bp z lewej\n",
    "                        \n",
    "                        #Sprawdzanie prawej i lewej strony alignmentu\n",
    "                        if len(set(binary_score_right_end)) == 1 and binary_score_right_end[0] == 1: #Sprawdza czy wszystkie elementy listy są takie same za pomocą setu \n",
    "                            #(set zwiera tylko unikalne wartości, więc jeśli elementy listy były takie same powinien być tylko 1)\n",
    "                            \n",
    "                            #Pętla od początku\n",
    "                            for k in indexer:\n",
    "                                if k != i:\n",
    "                                    seq3 = str(cons_seqs_df.at[k, 'Sequence'])\n",
    "                                    alignment2 = str(pairwise2.align.globalxx(seq1, seq3, one_alignment_only=True))\n",
    "                                    search4 = re.search('(?<=seqB=\\')[ACTG\\-]+\\'', alignment)\n",
    "                                    search5 = re.search('(?<=score=)[0-9]+', alignment)\n",
    "                                    search6 = re.search('(?<=end=)[0-9]+', alignment)\n",
    "                                    aligned_seq_l = search4.group(0)\n",
    "                                    score_l = int(search5.group(0))\n",
    "                                    align_length_l = int(search6.group(0))\n",
    "                                \n",
    "                                    identity_l = score/align_length_l*100\n",
    "                                    if identity_l < identity_threshold:\n",
    "                                        seq2_list_l = list(aligned_seq_l)\n",
    "                                        # Tworzy binary score\n",
    "                                        binary_score_l = []\n",
    "                                        for base in seq2_list_l:\n",
    "                                            if base == 'A' or base == 'T' or base == 'C' or base == 'G':\n",
    "                                                binary_score_l.append(1)\n",
    "                                            else:\n",
    "                                                binary_score_l.append(0)\n",
    "                                            \n",
    "                                        binary_score_left_end2 = binary_score_l[-min_chimera_length:]\n",
    "                                    \n",
    "                                        if len(set(binary_score_left_end2)) == 1 and binary_score_left_end2[0] == 1:\n",
    "                                            \n",
    "                                            #Sekwencja jest chimerą\n",
    "                                            reversed_df.at[j, 'Chimera'] = 'chimera'\n",
    "                            \n",
    "                            \n",
    "                        elif len(set(binary_score_left_end)) == 1 and binary_score_left_end[0] == 1:\n",
    "                            #Pętla od początku\n",
    "                            #Pętla od początku\n",
    "                            for k in indexer:\n",
    "                                if k != i:\n",
    "                                    seq3 = str(cons_seqs_df.at[k, 'Sequence'])\n",
    "                                    alignment2 = str(pairwise2.align.globalxx(seq1, seq3, one_alignment_only=True))\n",
    "                                    search4 = re.search('(?<=seqB=\\')[ACTG\\-]+\\'', alignment)\n",
    "                                    search5 = re.search('(?<=score=)[0-9]+', alignment)\n",
    "                                    search6 = re.search('(?<=end=)[0-9]+', alignment)\n",
    "                                    aligned_seq_r = search4.group(0)\n",
    "                                    score_r = int(search5.group(0))\n",
    "                                    align_length_r = int(search6.group(0))\n",
    "                                \n",
    "                                    identity_r = score/align_length_r*100\n",
    "                                    if identity_r < identity_threshold:\n",
    "                                        seq2_list_r = list(aligned_seq_r)\n",
    "                                        # Tworzy binary score\n",
    "                                        binary_score_r = []\n",
    "                                        for base in seq2_list_r:\n",
    "                                            if base == 'A' or base == 'T' or base == 'C' or base == 'G':\n",
    "                                                binary_score_r.append(1)\n",
    "                                            else:\n",
    "                                                binary_score_r.append(0)\n",
    "                                            \n",
    "                                        binary_score_right_end2 = binary_score_r[-min_chimera_length:]\n",
    "                                    \n",
    "                                        if len(set(binary_score_right_end2)) == 1 and binary_score_right_end2[0] == 1:\n",
    "                                            \n",
    "                                            #Sekwencja jest chimerą\n",
    "                                            reversed_df.at[j, 'Chimera'] = 'chimera'\n",
    "            \n",
    "            # Filtrowanie po chimerach, przywracanie właściwej kolejności po głębi i przywrócenie właściwych indeksów\n",
    "            filtered_seqs = reversed_df.drop(reversed_df[reversed_df.Chimera == 'chimera'].index)\n",
    "            filtered_seqs.sort_values(['Depth'], inplace=True)\n",
    "            cons_seqs_df = filtered_seqs.reset_index(drop=True)\n",
    "            \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p10 = f'{p1}/filtered'\n",
    "os.mkdir(p10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_allels = pd.DataFrame(columns = ['Amplicon', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'])\n",
    "\n",
    "files = 0\n",
    "for filename in os.scandir(p9):\n",
    "    if filename.is_dir():\n",
    "        files = files+1\n",
    "        \n",
    "no_files = [i for i in range(1, files+1)]\n",
    "\n",
    "for i in no_files:\n",
    "    con_seqs_dir = f'{p9}/amplicon_{i}/consensus_seqs.csv'\n",
    "    cons_seqs_df = pd.read_csv(con_seqs_dir)\n",
    "    be = Filter()\n",
    "    \n",
    "    depth_of_amplicon = cons_seqs_df['Depth'].sum()  # Wylicza głębię całego amplikonu dla sekwencji konsensusowych\n",
    "    cons_seqs_df['Frequency'] = cons_seqs_df['Depth']/depth_of_amplicon*100 # Wylicza częstość występowania sekwencji w amplikonie\n",
    "    \n",
    "    #Odrzuca sewkencje o zbyt niskiej głębi\n",
    "    cons_seqs_df.drop(cons_seqs_df[cons_seqs_df.Depth < min_amplicon_depth].index, inplace=True)\n",
    "    #Odrzuca sekwencje o zbyt niskiej czestotliwości\n",
    "    cons_seqs_df.drop(cons_seqs_df[cons_seqs_df.Frequency < min_per_amplicon_frequency].index, inplace=True)\n",
    "    \n",
    "#     #Odrzucenie sewkencji niekodujących    \n",
    "#     if discard_noncoding != None:\n",
    "#         be.noncoding(cons_seqs_df)\n",
    "    \n",
    "# #     #######################\n",
    "# #     print(cons_seqs_df)\n",
    "    \n",
    "    cons_seqs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \n",
    "    # CHIMERY\n",
    "    be.is_chimera(cons_seqs_df)\n",
    "\n",
    "    \n",
    "    #Odrzucanie nadmiernej ilości alleli w amplikonie\n",
    "    seqs_count = len(cons_seqs_df)\n",
    "    if seqs_count > max_allele_number:\n",
    "        cons_seqs_df = cons_seqs_df[:max_allele_number]\n",
    "\n",
    "    \n",
    "    #Dodanie kolumny z nazwą amplikonu\n",
    "    cons_seqs_df.insert(0, 'Amplicon', i)\n",
    "    \n",
    "    #Dodanie linijki\n",
    "    all_allels = all_allels.append(cons_seqs_df)\n",
    "    \n",
    "    #Stworzenie ostatecznego pliku fasta z allelami dla danego amplikonu\n",
    "        \n",
    "    outpath = f'{p10}/amplicon{i}.fasta'\n",
    "    with open(outpath, 'w') as outfile:\n",
    "        for index, row in cons_seqs_df[ ['ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "            outfile.write('>' + row.ID + ' | depth: ' + str(row. Depth) + ' | length: ' + str(row.Length) + ' | frequency: ' + str(row.Frequency) + '\\n' + row.Sequence + '\\n')\n",
    "    \n",
    "all_allels.reset_index(drop=True, inplace=True)\n",
    "whole_depth =  all_allels['Depth'].sum()  \n",
    "\n",
    "droped_duplcates = all_allels.drop_duplicates(subset=['Sequence'])\n",
    "labels = droped_duplcates['Sequence'].tolist()\n",
    "\n",
    "#all_allels.groupby(labels)\n",
    "\n",
    "final_allels = pd.DataFrame(columns = ['Amplicon', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency', 'Frequency_across_amplicons'])\n",
    "final_stats = pd.DataFrame(columns = ['Sequence', 'Amplicons', 'Count', 'Depth', 'Frequency_across_amplicons'])\n",
    "\n",
    "# len_df = len(all_allels)\n",
    "# iterator = [i for i in range(len_df)]\n",
    "\n",
    "for label in labels:\n",
    "    allel_df = pd.DataFrame(columns = ['Amplicon', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency', 'Frequency_across_amplicons'])\n",
    "    allel_stats = pd.DataFrame(columns = ['Sequence', 'Amplicons', 'Count', 'Depth', 'Frequency_across_amplicons'])\n",
    "    \n",
    "    for index, row in all_allels[ ['Amplicon', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "#     for x in iterator:\n",
    "        if all_allels.at[index, 'Sequence'] == label:\n",
    "        #if row.Sequence == label:\n",
    "            allel_df = allel_df.append(all_allels.iloc[index])\n",
    "    \n",
    "    allel_df.reset_index(drop=True, inplace=True)\n",
    "    allel_depth = allel_df['Depth'].sum() #Głębia allela na przestrzeni wszystkich amplikonów\n",
    "    allel_df['Frequency_across_amplicons'] = allel_depth/whole_depth*100 #Częstość występowania na przestrzeni wszystkich ampikonów\n",
    "    count = len(allel_df) #Ilość wystąpień danego allela we wszustkich amplikonach\n",
    "    \n",
    "    amplicon_list = [] \n",
    "    for index, row in allel_df[ ['Amplicon', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency', 'Frequency_across_amplicons'] ].iterrows():\n",
    "        amplicon_list.append(row.Amplicon)\n",
    "    \n",
    "    allel_stats.at[0, 'Sequence'] = allel_df.at[0, 'Sequence']\n",
    "    allel_stats.at[0, 'Amplicons'] = str(amplicon_list)\n",
    "    allel_stats.at[0, 'Count'] = count\n",
    "    allel_stats.at[0, 'Depth'] = allel_depth\n",
    "    allel_stats.at[0, 'Frequency_across_amplicons'] = allel_depth/whole_depth*100\n",
    "    \n",
    "    final_stats = final_stats.append(allel_stats)\n",
    "    final_stats.sort_values(['Frequency_across_amplicons'], ascending=False, inplace=True)\n",
    "    \n",
    "    #Dołącznie allela z częstością do df końcowego\n",
    "    final_allels = final_allels.append(allel_df)\n",
    "    final_allels.sort_values(['Amplicon'], inplace=True)\n",
    "    \n",
    "# Stworzenie outputu fasta dla ostatecznych sekwencji\n",
    "outpath = f'{p10}/all_allels.fasta'\n",
    "with open(outpath, 'w') as outfile:\n",
    "    for index, row in final_allels[ ['Amplicon', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency', 'Frequency_across_amplicons'] ].iterrows():\n",
    "        outfile.write('>' + 'Amplicon: ' + str(row.Amplicon) + ' | ' + row.ID + ' | depth: ' + str(row. Depth) + ' | length: ' + str(row.Length) + ' | frequency: ' + str(row.Frequency) + ' | frequency across amplicons: ' + str(row.Frequency_across_amplicons) + '\\n' + row.Sequence + '\\n')\n",
    "\n",
    "#Stworzenie outputu tsv dla ostatecznych sekwencji\n",
    "outpath = f'{p10}/all_allels_stats.csv'\n",
    "final_stats.to_csv(outpath, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplicon</th>\n",
       "      <th>ID</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Length</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Frequency_across_amplicons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Amplicon, ID, Sequence, Depth, Length, Frequency, Frequency_across_amplicons]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_allels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplicon</th>\n",
       "      <th>ID</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Length</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Chimera</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Amplicon, ID, Sequence, Depth, Length, Frequency, Chimera]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_seqs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = 0\n",
    "for filename in os.scandir(p9):\n",
    "    if filename.is_file():\n",
    "        files = files+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1 = Seq(\"ACCGGT\")\n",
    "seq2 = Seq(\"ACGGGT\")\n",
    "alignments = pairwise2.align.globalxx(seq1, seq2, one_alignment_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alignment(seqA='ACCGG-T', seqB='A-CGGGT', score=5.0, start=0, end=7)]\n"
     ]
    }
   ],
   "source": [
    "alignments =str(alignments)\n",
    "print(alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = re.search('(?<=seqB=\\')[ACTG\\-]+', alignments)\n",
    "search1 = re.search('(?<=score=)[0-9]+', alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.group(0)\n",
    "search1.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'C', 'C', 'G', 'G', 'T']\n",
      "['A', 'C', 'C', 'G', 'G', 'T']\n"
     ]
    }
   ],
   "source": [
    "a = list(seq1)\n",
    "print(a)\n",
    "b = str(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def noncoding(self, cons_seqs_df):\n",
    "        pattern1 = '[ATCG]*TAG[ATCG]*'\n",
    "        pattern2 = '[ATCG]*TAA[ATCG]*'\n",
    "        pattern3 = '[ATCG]*TGA[ATCG]*'\n",
    "        patterns = [pattern1, pattern2, pattern3]\n",
    "        index_values = list(cons_seqs_df.index.values)\n",
    "        \n",
    "        for index in index_values:\n",
    "            for pattern in patterns:\n",
    "                seq = cons_seqs_df.at[index, 'Sequence']\n",
    "                checked_seq = re.sub(pattern, '', seq)\n",
    "                cons_seqs_df.at[index, 'Sequence'] = checked_seq        \n",
    "        \n",
    "        cons_seqs_df.drop(cons_seqs_df[cons_seqs_df.Sequence == ''].index, inplace=True)\n",
    "        cons_seqs_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 'CTGCGACCTCAGGAATACCAGACTTCCCAGAGTTTGTGACTGTTGGGTTGGTGAATGGAGAACCCATCTCGTACTATGACAGCATCATCCGCAGAGAAACTCCCCGACAGGACTGGATGGCCAAGACTGAGGGGTCTGACTACTGGGAGAGTCAGACTCAGATCTCCATTGGTTCCGAACAGACCTTCAAAGCCAACATTGATGTTGCCAAGCA'\n",
    "\n",
    "#pattern1 = '[ATCG]*TAG[ATCG]*'\n",
    "pattern2 = '[ATCG]*TAA[ATCG]*'\n",
    "pattern = '[ATCG]*TGA[ATCG]*'\n",
    "patterns = [pattern1, pattern2, pattern3]\n",
    "\n",
    "#for pattern in patterns:\n",
    "checked_seq = re.sub(pattern, '', seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
