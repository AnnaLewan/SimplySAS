{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import zipfile\n",
    "import tarfile\n",
    "from collections import defaultdict\n",
    "import uuid\n",
    "import subprocess\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_multifile = 'Des_ex3.zip'\n",
    "input_csv = 'startery.csv'\n",
    "min_length = 100\n",
    "expected_len = 176\n",
    "substitution_error_threshold = 1.56\n",
    "min_amplicon_seq_frequency = 38.11\n",
    "min_dominant_frequency_threshold = 43.64\n",
    "\n",
    "p1 = f'ampliSAS_analysis'\n",
    "os.makedirs(p1)\n",
    "\n",
    "p2 = f'ampliSAS_analysis/input'\n",
    "os.makedirs(p2)\n",
    "\n",
    "if tarfile.is_tarfile(input_multifile) or zipfile.is_zipfile(input_multifile) != True:\n",
    "    raise Exception('File is not a multifile')\n",
    "            \n",
    "if os.path.isfile(input_csv) != True:\n",
    "    raise Exception('CSV file not existing')\n",
    "    \n",
    "primer_file = pd.read_csv(input_csv, names=[0,1])\n",
    "\n",
    "forward = primer_file.iloc[1, 1]\n",
    "reverse = str(Seq(primer_file.iloc[2, 1]).reverse_complement())\n",
    "        \n",
    "ID = primer_file.iloc[0, 0]\n",
    "forward_ID = f'{ID}_forward'\n",
    "reverse_ID = f'{ID}_reverse'\n",
    "\n",
    "p3 = f'ampliSAS_analysis/input/fasta_primers'\n",
    "os.mkdir(p3) \n",
    "   \n",
    "outpath_f = f'{p3}/starter_f.fasta'\n",
    "outpath_r = f'{p3}/starter_r.fasta'\n",
    "with open(outpath_f, 'w') as outfile:\n",
    "    outfile.write('>' + forward_ID + '\\n' + forward)\n",
    "with open(outpath_r, 'w') as outfile:\n",
    "    outfile.write('>' + reverse_ID + '\\n' + reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fastqs from multifile\n",
      "Extraction finished\n"
     ]
    }
   ],
   "source": [
    "#os.mkdir(f'{p2}/fastqs')\n",
    "print(f'Extracting fastqs from multifile')\n",
    "\n",
    "#for i in input_multifile:\n",
    "with zipfile.ZipFile(input_multifile, 'r') as zip:\n",
    "    zip.extractall(path = f'{p2}')\n",
    "        \n",
    "print(f'Extraction finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastqs_folder = re.sub('\\.zip', '', input_multifile)\n",
    "\n",
    "os.rename(f'{p2}/{fastqs_folder}', f'{p2}/fastqs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Amplicon:\n",
    "    def __init__(self, number):\n",
    "        self.number = number\n",
    "        self.clusters = None\n",
    "        self.seqs_count = None\n",
    "        \n",
    "        \n",
    "    def cut_primers(self, input_fastq, outputs_dir_good, outputs_dir_bad, starter_f, starter_r):\n",
    "        command = f'python3 cutPrimers.py -r1 {input_fastq} -pr15 {starter_f} -pr13 {starter_r} -tr1 {outputs_dir_good}/amplicon{self.number}_primerfree.fastq -utr1 {outputs_dir_bad}/odrzucone_ampli{self.number}.fastq'        \n",
    "        process = subprocess.check_output(command, shell=True)\n",
    "        \n",
    "            \n",
    "    def parse_sequence_file(self, inputfastq_primerfree):\n",
    "        global amplicon_table\n",
    "        amplicon_table = pd.DataFrame(columns = ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'])      ## ogarnąc czy przypadkiem nie trzeba zrobić w inicie  \n",
    "        \n",
    "        dedup_records = defaultdict(list) #tworzy słownik, dla których wartościami jest lista\n",
    "\n",
    "        for record in SeqIO.parse(inputfastq_primerfree, \"fastq\"):\n",
    "            dedup_records[str(record.seq)].append(record.id) #tworzy słownik, dla którego key=seq, a value=id. Na tym etapie dochodzi do odnalezienia duplikatów - jest ich tyle wartości id\n",
    "        for seq, ids in sorted(dedup_records.items(), key=lambda t: len(t[1]), reverse=True): #ta linijka służy przesortowaniu sekwencji pod względem ich głębokości                \n",
    "            id_hold = ids[0]\n",
    "            id_len = len(ids)\n",
    "            seq_len = len(seq)\n",
    "            unique_hash = str(uuid.uuid1())\n",
    "            amplicon_table = amplicon_table.append({'Hash' : unique_hash, 'ID' : id_hold, 'Sequence' : seq, 'Depth' : id_len, 'Length' : seq_len}, ignore_index = True)\n",
    "        \n",
    "        self.seqs_count = len(amplicon_table) # Ilość sekwencji w amplikonie\n",
    "        depth_of_amplicon = amplicon_table['Depth'].sum() #Wylicza głębie całego amplikonu\n",
    "        \n",
    "        for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "            freq_in_amplicon = row.Depth/depth_of_amplicon*100\n",
    "            row.Frequency_per_amplicon = freq_in_amplicon\n",
    "            \n",
    "        \n",
    "        return amplicon_table\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def create_fasta(self, inputs_dir):\n",
    "        outpath = f'{inputs_dir}/{self.number}_amplicon.fasta'\n",
    "        with open(outpath, 'w') as outfile:\n",
    "            for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "                outfile.write('>' + row.Hash + ' | ' + row.ID + ' | depth: ' + str(row.Depth) + ' | length: ' + str(row.Length) + ' | frequency per amplicon: ' + str(row.Frequency) + '\\n' + row.Sequence + '\\n')\n",
    "                \n",
    "                \n",
    "    #Niesprawdzone\n",
    "    def create_seq_fasta(self, seqs_fasta_dir):\n",
    "        for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "            outpath = f'{seqs_fasta_dir}/{row.Hash}.fasta'\n",
    "            with open(outpath, 'w') as outfile:\n",
    "                outfile.write('>' + row.Hash + ',' + row.ID + ',' + str(row.Depth) + ',' + str(row.Length) + ',' + str(row.Frequency) + '\\n' + row.Sequence + '\\n')\n",
    "\n",
    "    #Koniec niesprawdzonego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, amplicon, min_length, expected_len):\n",
    "        self.dominant_seq = None\n",
    "        self.amplicon = amplicon\n",
    "        self.min_length = min_length\n",
    "        self.expected_len = expected_len\n",
    "        \n",
    "        \n",
    "    def find_dominant(self):\n",
    "#        amplicon_table.sort_values('Lenght', ascending=False, inplace=True)\n",
    "        amplicon_table.drop(amplicon_table[amplicon_table.Length < self.min_length].index, inplace=True)  #odrzuca sekwencje o długości niższej niż zadana przez użytkownika\n",
    "        amplicon_table.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        \n",
    "        global cluster_dom\n",
    "        cluster_dom = pd.DataFrame(columns = ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'])\n",
    "#        if amplicon_table[amplicon_table.Length == self.expected_length].iloc[0]:\n",
    "#            dominant_seq = amplicon_table.iloc[0]\n",
    "#        else:\n",
    "        for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "            if row.Length == self.expected_len:\n",
    "                dominant_seq = amplicon_table.iloc[0]\n",
    "                cluster_dom = cluster_dom.append( [dominant_seq] )\n",
    "                amplicon_table.drop(0)\n",
    "                amplicon_table.reset_index(drop=True, inplace=True)\n",
    "                break\n",
    "                       \n",
    "                \n",
    "    def seq2seq(self, seq1, seq2, substitution_error_threshold, temp_dir, number):\n",
    "        command = f'./fogsaa {seq1} {seq2} 1 0 1 -1 -1'\n",
    "        process = subprocess.check_output(command, shell=True) #Przykładowy wynik: b'176\\n176\\nElapsed time: 1 milliseconds\\ntotal nodes expanded==176\\n\\nscore= 174\\n'\n",
    "        process = process.decode('utf-8') #Zamiana bajtów na str\n",
    "\n",
    "        outpath = f'{temp_dir}/{number}.txt'\n",
    "        with open(outpath, 'w') as outfile:\n",
    "            outfile.write(process)\n",
    "                \n",
    "        score = int(re.sub('score=\\ ', '', ' '.join(map(str, re.findall('score=\\ .[0-9]*', process))))) # Wyodrębnienie score dla alignmentu\n",
    "        len_seqs = int(re.sub('score=\\ ', '', ' '.join(map(str, re.findall('(?<=[0-9]\\s)[0-9]+', process))))) # Wyodrębnia długość porównywanych sekwencji - założenie, że porównywane sekwencje są tylko tej samej długości\n",
    "        \n",
    "        # Duży problem a propos wyliczenia substitution_error. Fogsaa nie zwraca jaka część score to kara za mismatch.\n",
    "        # Oznacza to, że nie wiem ile substytucji tak naprawdę zaszło, a bez tego nie mogę wyliczyć błędu substytucji.\n",
    "        \n",
    "        global substitution_error\n",
    "        substitution_error = (len_seqs - score)/len_seqs*100 # Wylicza poziom błedów dla danego alignmentu\n",
    "        \n",
    "        \n",
    "    def multiple_aline_seqs(self, cluster_df, clusters_dir, mafftout_dir, number):\n",
    "        #Tworzenie wspólnej fasty dla klastra\n",
    "        outpath = f'{clusters_dir}/{number}_cluster.fasta'\n",
    "        with open(outpath, 'w') as outfile:\n",
    "            for index, row in cluster_df[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "                outfile.write('>' + row.Hash + ' | ' + row.ID + ' | depth: ' + str(row.Depth) + ' | length: ' + str(row.Length) + ' | frequency per amplicon: ' + str(row.Frequency) + '\\n' + row.Sequence + '\\n')\n",
    "                        \n",
    "        #Mafft na pliku --> nie jest potrzebny jesli do klastra trafiają tylko sekwencje o tej samej długości\n",
    "       # command = f'mafft --auto --quiet {outpath} > {mafftout_dir}'\n",
    "       # process = subprocess.check_output(command, shell=True)\n",
    "        \n",
    "        # Stworzenie dataframe na sekwencję konsensusową z danymi\n",
    "        global consensus_df\n",
    "        \n",
    "        consensus_df = pd.DataFrame(columns = ['ID', 'Sequence', 'Depth', 'Length'])\n",
    "        id_cluster = f'cluster_{number}'\n",
    "        len_cluster = cluster_df.loc[0, 'Length']\n",
    "        depth_cluster = cluster_df['Depth'].sum()\n",
    "               \n",
    "        #Ustalenie konsensusowej\n",
    "        align = AlignIO.read(outpath, 'fasta')\n",
    "        summary_align = AlignInfo.SummaryInfo(align)\n",
    "        consensus = str(summary_align.dumb_consensus())\n",
    "    #    my_pssm = summary_align.pos_specific_score_matrix(consensus,chars_to_ignore = ['N']) # Matrix z częstością dla danej pozycji \n",
    "    \n",
    "        consensus_df = consensus_df.append({'ID' : id_cluster, 'Depth' : depth_cluster, 'Length' : len_cluster, 'Sequence' : consensus}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = f'{p2}/fastqs'\n",
    "p5 = f'{p2}/fastqs_primerfree'\n",
    "p6 = f'{p2}/fastqs_primerfree/out'\n",
    "os.mkdir(p5)\n",
    "os.mkdir(p6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wyciananie starterów\n",
    "files = 0\n",
    "for filename in os.scandir(p4):\n",
    "    files = files+1\n",
    "    if filename.is_file():\n",
    "        go = Amplicon(files)\n",
    "        go.cut_primers(filename.path, p5, p6, outpath_f, outpath_r)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(p6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "p7 = f'{p2}/fasta'\n",
    "os.mkdir(p7)\n",
    "\n",
    "p8 = f'{p1}/amplicons_filtered_seqs' # Tu będa znajdowały się foldery dla każdego amplikonu zawierające każdą sekwencję w amplikonie jako osobny plik fasta. Sekwencje te są już przefiltrowane względem długości, głebi itd\n",
    "os.mkdir(p8)\n",
    "\n",
    "p9 = f'{p1}/clusters'\n",
    "os.mkdir(p9)\n",
    "\n",
    "p10 = f'{p1}/clusters/mafftout'\n",
    "os.mkdir(p10)\n",
    "\n",
    "temp_dir = f'{p1}/blad'\n",
    "os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_length = 100\n",
    "# expected_len = 176\n",
    "# substitution_error_threshold = 1.56\n",
    "# min_amplicon_seq_frequency = 38.11\n",
    "# min_dominant_frequency_threshold = 43.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = 0\n",
    "for filename in os.scandir(p5):\n",
    "    files = files+1\n",
    "    if filename.is_file():\n",
    "        go = Amplicon(files)\n",
    "        go.parse_sequence_file(filename.path)\n",
    "        go.create_fasta(p7) #SPRAWDZONE\n",
    "        do = Cluster(files, min_length, expected_len)   #Wczytanie pliku z funkcjami klastrowania\n",
    "        do.find_dominant()\n",
    "        seqs_fasta_dir = f'{p8}/amplicon_{files}'\n",
    "        os.mkdir(seqs_fasta_dir)\n",
    "        go.create_seq_fasta(seqs_fasta_dir)\n",
    "        clusters_dir = f'{p9}/amplicon_{files}'\n",
    "        os.mkdir(clusters_dir)\n",
    "        mafftouts = f'{p10}/amplicon_{files}'\n",
    "        os.mkdir(mafftouts)\n",
    "        all_clusters = pd.DataFrame(columns = ['ID', 'Sequence', 'Depth', 'Length'])\n",
    "        subdominants = 0 #zmienna do pilnowania liczby klastrów poza dominującym\n",
    "        subdominants_df =  pd.DataFrame(columns = ['Subdominant', 'Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency']) #Tworzy dataframe, w którym będzie zapisywana każda sekwencja subdominująca z danego ampilkonu\n",
    "        \n",
    "        #Pierwsze przejście - dla sekwencji domminującej\n",
    "        for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "            #if index != 0: #po dodaniu dropa w find_dominant() niepotrzebne\n",
    "                seq1a = cluster_dom.loc[0, 'Hash']\n",
    "                seq1 = f'{seqs_fasta_dir}/{seq1a}.fasta'\n",
    "                seq2 = f'{seqs_fasta_dir}/{row.Hash}.fasta'           \n",
    "                do.seq2seq(seq1, seq2, substitution_error_threshold, temp_dir, index)\n",
    "                if substitution_error < substitution_error_threshold:\n",
    "                    #Warunek freq\n",
    "                    freq_to_dom = row.Depth/cluster_dom.loc[0, 'Depth']*100\n",
    "                    if row.Frequency < min_amplicon_seq_frequency or freq_to_dom < min_dominant_frequency_threshold:\n",
    "                        if row.Length == cluster_dom.loc[0, 'Length']:\n",
    "                            cluster_dom = cluster_dom.append(amplicon_table.iloc[index]) #dodaje sekwencję do klastra dominującego\n",
    "                            amplicon_table.drop(index) #usuwa sekwencję z głównego dataframe amplikonu\n",
    "                            do.multiple_aline_seqs(cluster_dom, clusters_dir, mafftouts, 'dom')\n",
    "                    else:\n",
    "                        subdominants += 1\n",
    "                        subdominants_df = subdominants_df.append(amplicon_table.iloc[index])\n",
    "                        subdominants_df = subdominants_df.append({'Subdominant' : subdominants}, ignore_index = True)\n",
    "                        amplicon_table.drop(index)\n",
    "        all_clusters = all_clusters.append(consensus_df.iloc[0])\n",
    "        # Na tym kończy się pierwsze przejście przez amplikon. Jego efektem jest wytworzenie klastra,\n",
    "        # który jako podstawe ma sekwencję dominującą w amplikonie oraz usunięcie z amplicon_table\n",
    "        # zarówno sekwencji dominującej jak i pozostałych należących do klastra sekwencji.\n",
    "        # Dodatkowo tworzony jest DataFrame, który przechowuje wszystkie sekwencje, które zostały\n",
    "        # uznane za subdominujące i mogą zostac podstawami nowych klastrów\n",
    "        \n",
    "#         for index, row in subdominants_df[ ['Subdominant', 'Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "#             dom_sub_seq_cluster = pd.DataFrame(columns = ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'])\n",
    "#             dom_sub_seq = subdominants_df.iloc[0]\n",
    "#             dom_sub_seq_cluster = dom_sub_seq_cluster.append( [dom_sub_seq] )\n",
    "#             seq1a = dom_sub_seq.loc[0, 'Hash']\n",
    "#             seq1 = f'{seqs_fasta_dir}/{seq1a}.fasta'\n",
    "#             for index, row in subdominants_df[ ['Subdominant', 'Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "#                 if index != 0:\n",
    "#                     if row.Length == dom_sub_seq.loc[0, 'Length']:\n",
    "#                         seq2 = f'{seqs_fasta_dir}/{row.Hash}.fasta'\n",
    "#                         do.seq2seq(seq1, seq2, substitution_error_threshold)\n",
    "#                         if substitution_error < substitution_error_threshold:\n",
    "#                             dom_sub_seq_cluster = dom_sub_seq_cluster.append(subdominants_df.iloc[index])\n",
    "#                             subdominants_df.drop(subdominants_df.iloc[index])\n",
    "#                             subdominants_df.reset_index(drop=True, inplace=True)\n",
    "#             for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length', 'Frequency'] ].iterrows():\n",
    "#                 dom_sub_seq = dom_sub_seq_cluster.iloc[0]\n",
    "        \n",
    "        #Stworzenie ostatecznego pliku fasta z sekwencjami konsensusowymi dla danego klastra\n",
    "        \n",
    "        outpath = f'{clusters_dir}/consensus_seqs.fasta'\n",
    "        with open(outpath, 'w') as outfile:\n",
    "            for index, row in all_clusters[ ['ID', 'Sequence', 'Depth', 'Length'] ].iterrows():\n",
    "                outfile.write('>' + row.ID + ' | depth: ' + str(row. Depth) + ' | length: ' + str(row.Length) + '\\n' + row.Sequence + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in amplicon_table[ ['Hash', 'ID', 'Sequence', 'Depth', 'Length'] ].iterrows():\n",
    "#     if index != 0:\n",
    "#         seq1a = cluster_dom.loc[0, 'Hash']\n",
    "#         print(seq1a)\n",
    "#         #print(amplicon_table.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
